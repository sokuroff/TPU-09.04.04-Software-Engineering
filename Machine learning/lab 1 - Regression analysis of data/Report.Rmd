---
title: "Лабораторная работа 1. Регрессионный анализ данных."
author: 'Сокуров Р.Е.'
date: "13.09.2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Задание
Целью данной работы является формирование у магистрантов знаний в области статистики и навыков программирования на языке программирования R при решении регрессионных задач на основе набора данных.

1. Изучите Статью <https://www.r-bloggers.com/2015/09/fitting-a-neural-network-in-r-neuralnet-package/>.

2. Выполните расчеты по Статье, сравните два метода и проведите перекрестную проверку (cross validation) на языке программирования R.

3. Выполните аналогичные расчеты для примера выбранного самостоятельно (https://www.kaggle.com, datasetsearch) на языке программирования R.

4. Отчет сформируйте в RStudio в pdf-формате и прикрепите в качестве ответа.

# Ход работы

## 1. Выполнение расчётов по статье.

### 1.1. Линейная регрессия
Линейная регрессия — это метод анализа данных, который предсказывает неизвестные данные с помощью известных значения данных. Он математически моделирует неизвестную или зависимую переменную и известную или независимую переменную в виде линейного уравнения.

Согласно статье, для работы был выбран датасет `Boston` из библиотеки `MASS` и выполнена проверка целостности данных:

```{r}
set.seed(500) # установка зерна случайной генерации
library(MASS) # подключение библиотеки с датасетами
data <- Boston # в data кладём данные с датасета Boston

apply(data,2,function(x) sum(is.na(x))) # проверка целостности данных
```

Затем полученный набор данных был разделён на обучающий и тестовый наборы в соотношении 75/25%:
```{r}
index <- sample(1:nrow(data),round(0.75*nrow(data))) # выбираем случайные индексы 75% датасета
train <- data[index,] # 75% датасета для обучения
test <- data[-index,] # остальное (25%) для теста
```

После чего была создана модель линейной регрессии для предсказания средней стоимости дома в тысячах долларах (medv в дальнейшем), обучена и протестирована.
```{r}
lm.fit <- glm(medv~., data=train) # создание модели линейной регрессии для предсказания medv
pr.lm <- predict(lm.fit, test) # кладём в pr.lm предсказывания на тестовом наборе
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test) # считаем MSE между реальностью и предсказ. знач.
summary(lm.fit) # отчёт о полученной модели
```

### 1.2. Нейронная сеть
Нейронная сеть — математическая модель, а также ее программное или аппаратное воплощение, построенная по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма. 

Перед созданием и обучением нейронной сети, была выполнена нормализация данных в датасете в диапазон [0, 1]:
```{r}
maxs <- apply(data, 2, max) # забираем из каждого столбца его max значение
mins <- apply(data, 2, min) # забираем из каждого столбца его min значение

scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # нормализуем данные в интервал [0, 1]

train_ <- scaled[index,] # часть для обучения: нормализованные 75% сета
test_ <- scaled[-index,] # тест: нормализованные 25% сета
```

Затем была создана и обучена модель многослойного перцептрона (MLP)
```{r}
library(neuralnet) # подключаем библиотеку с нейронной сетью
n <- names(train_) # копируем имена столбцов
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + "))) # формируем формулу для предсказывания medv на основе всех остальных данных
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T) # создаём и обучаем MLP 
plot(nn) # и выводим
```
```{r nn, echo=FALSE}
plot(nn, rep="best", render=TRUE)
```

Полученная модель была использована для предсказания средней стоимости дома в тысячах долларах (medv в дальнейшем). Также была посчитана MSE.
```{r}
pr.nn <- compute(nn,test_[,1:13]) # прогноз нейросети на тестовой выборке на первых 13 столбцах (без medv)
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv) # возвращаем нормализованные [0, 1] предсказания обратно [min, max]
test.r <- (test_$medv)*(max(data$medv)-min(data$medv))+min(data$medv) # то же самое, что и в прошлой строчке, только для medv
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_) # считаем MSE
```
### 1.3 Сравнение линейной регрессии и нейронной сети
Значения MSE двух моделей: 
```{r}
print(paste(MSE.lm,MSE.nn)) # выводим MSE LM и MSE NN
```

Затем были построены два графика (для каждой модели), где по оси X были отложены реальные значения medv, а по оси Y предсказанные значения. Таким образом, идеальную модель описывает чёрная линия на графиках. 

```{r}
par(mfrow=c(1,2)) # создаём в одном окне 2 графика

# Построение графиков, которые позволяют визуализировать, 
# насколько близки прогнозы моделей к фактическим значениям.
plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')

plot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
```

По итогам сравнения можно сделать вывод о том, что нейронная сеть лучше справляется с предсказанием значений medv, поскольку значение MSE у неё меньше, что также подтверждается визуальной интерпретацией полученных графиков.

### 1.4 Кросс-валидация линейной регрессионной модели
Кросс-валидация — это метод оценки производительности модели машинного обучения, используемый для проверки её способности обобщать данные.
```{r}
library(boot) # загружаем библиотеку для кросс-валидации
set.seed(200) # установка зерна случайной генерации
lm.fit <- glm(medv~.,data=data) # вновь создаём модель линейной регрессии
cv.glm(data,lm.fit,K=10)$delta[1] # выполняем кросс-валидацию на 10 фолдах
```
### 1.5 Кросс-валидация нейронной сети
Примечание: для корректного отображения результаты выполнения следующих двух ячеек были скрыты из отчёта.
```{r results='hide'}
set.seed(450) # меняем зерно случайных чисел
cv.error <- NULL # создаём переменную для хранения ошибки кросс-валидации
k <- 10 # количество фолдов
library(plyr) # библиотека для отображения прогресс-баров
pbar <- create_progress_bar('text') # выбираем текстовый прогресс бар
pbar$init(k) # инициализируем его с максимальным количеством шагов равным k
```

```{r results='hide'}
for(i in 1:k){ # цикл с 1 до k
  index <- sample(1:nrow(data),round(0.9*nrow(data))) # делим выборку 90/10%
  train.cv <- scaled[index,] # 90%
  test.cv <- scaled[-index,] # 10%
  
  nn <- neuralnet(f,data=train.cv,hidden=c(5,2),linear.output=T) # обучаем нейр. сеть
  
  pr.nn <- compute(nn,test.cv[,1:13]) # вычисляем прогнозы на тестовых данных
  pr.nn <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv) # обратная нормализация
  
  test.cv.r <- (test.cv$medv)*(max(data$medv)-min(data$medv))+min(data$medv) # обратная нормализация
  
  cv.error[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv) # сохраняем MSE текущего фолда
  
  pbar$step() # шаг для прогресс-бара
}
```

После чего были выведены средняя ошибка кросс-валидации и весь набор ошибок на каждом шаге:
```{r}
mean(cv.error) # считаем среднюю ошибку кросс-валидаци
cv.error # вывод полного вектора ошибок (ошибки каждого шага)
```

```{r}
boxplot(cv.error,xlab='MSE CV',col='cyan',
        border='blue',names='CV error (MSE)',
        main='CV error (MSE) for NN',horizontal=TRUE)
```

График показывает разброс ошибок MSE для нейронной сети, где медиана находится ближе к 7, а диапазон значений распределен от около 5 до 13. Левая граница прямоугольника находится в районе 6, что соответствует первому квартилю распределения. Правая граница прямоугольника находится в районе 10, что соотвествутет третьему квартилю распределения.

## 2. Выполнение расчётов по выбранной теме
### 2.1 Линейная регрессия
Для самостоятельной части работы был выбран датасет `Car information dataset` <https://www.kaggle.com/datasets/tawfikelmetwally/automobile-dataset>. Предполагается предсказание величины Acceleration - способности разгоняться, измеренной в секундах.

Перед созданием модели необходимо обработать данные: удалить ненужные столбцы (например столбец name характеризующий название автомобиля), а также удалить все строки, в которых отсутствуют данные.


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

```{r results='hide'}
data <- read.csv("Automobile.csv")

data <- data[, -c(1, ncol(data))] # удаляем ненужные стобцы типа названия автомобиля
data <- drop_na(data) # удаляем NA из датасета
```

Затем полученный набор данных был разделён на обучающий и тестовый наборы в соотношении 75/25%:
```{r}
index <- sample(1:nrow(data),round(0.75*nrow(data))) # забираем 75% случайных индексов записей
train <- data[index,] # 75% всех индексов идут на обучение
test <- data[-index,] # оставшаяся часть (25%) на тест
```

После чего была создана модель линейной регрессии для предсказания Acceleration, обучена и протестирована.
```{r}
lm.fit <- glm(acceleration~., data=train) # собираем модель линейной регрессии и обучаем её на train сете
summary(lm.fit) # отчёт о модели
pr.lm <- predict(lm.fit, test) # кладём в переменную предсказываемые lm значения acceleration 
MSE.lm <- sum((pr.lm - test$acceleration)^2)/nrow(test) # считаем MSE
```

### 2.2. Нейронная сеть
Перед созданием и обучением нейронной сети, была выполнена нормализация данных в датасете в диапазон [0, 1]:
```{r}
maxs <- apply(data, 2, max) # забираем из каждого столбца его max значение
mins <- apply(data, 2, min) # забираем из каждого столбца его min значение

scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # нормализуем данные в интервал [0, 1]

train_ <- scaled[index,] # часть для обучения: нормализованные 75% сета
test_ <- scaled[-index,] # тест: нормализованные 25% сета
```

Затем была создана и обучена модель многослойного перцептрона (MLP)
```{r}
library(neuralnet) # подключение библиотеки с нейросетями
n <- names(train_) # n - имена всех столбцов датасета
f <- as.formula(paste("acceleration ~", paste(n[!n %in% "acceleration"], collapse = " + "))) # генерация формулы из string
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T) # создаём MLP и обучаем его на train 
```
```{r nn2, echo=FALSE}
plot(nn, rep="best", render=TRUE)
```

Полученная модель была использована для предсказания acceleration. Также была посчитана MSE.
```{r}
test_data <- test_[, -which(names(test_) == "acceleration")]# выбираем все столбцы кроме acceleration

pr.nn <- neuralnet::compute(nn, test_data)
pr.nn_ <- pr.nn$net.result*(max(data$acceleration)-min(data$acceleration))+min(data$acceleration) # возвращаем нормализованные [0, 1] предсказания обратно [min, max]
test.r <- (test_$acceleration)*(max(data$acceleration)-min(data$acceleration))+min(data$acceleration) # то же самое, что и в прошлой строчке, только для quality
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_) # считаем MSE
```
### 2.3 Сравнение линейной регрессии и нейронной сети
Значения MSE двух моделей: 
```{r}
print(paste(MSE.lm,MSE.nn)) # выводим MSE LM и MSE NN
```

Затем были построены два графика (для каждой модели), где по оси X были отложены реальные значения acceleration, а по оси Y предсказанные значения. Таким образом, идеальную модель описывает чёрная линия на графиках. 
```{r}
par(mfrow=c(1,2)) # создаём в одном окне 2 графика

# Построение графиков, которые позволяют визуализировать, 
# насколько близки прогнозы моделей к фактическим значениям.
plot(test$acceleration,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')

plot(test$acceleration,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
```

По итогам сравнения можно сделать вывод о том, что нейронная сеть лучше справляется с предсказанием значений Acceleration, поскольку значение MSE у неё меньше, что также подтверждается визуальной интерпретацией полученных графиков. 
### 2.4 Кросс-валидация линейной регрессионной модели
```{r}
library(boot) # загружаем библиотеку для кросс-валидации
set.seed(200) # устанавливаем зерно случайных чисел
lm.fit <- glm(acceleration~.,data=data) # вновь создаём модель линейной регрессии
cv.glm(data,lm.fit,K=10)$delta[1] # выполняем кросс-валидацию на 10 фолдах
```
### 2.5 Кросс-валидация нейронной сети
Примечание: для корректного отображения результаты выполнения следующих двух ячеек были скрыты из отчёта.
```{r results='hide'}
set.seed(450) # меняем зерно случайных чисел
cv.error <- NULL # создаём переменную для хранения ошибки кросс-валидации
k <- 10 # количество фолдов
library(plyr) # библиотека для отображения прогресс-баров
pbar <- create_progress_bar('text') # выбираем текстовый прогресс бар
pbar$init(k) # инициализируем его с максимальным количеством шагов равным k
```

```{r results='hide'}
for(i in 1:k){ # цикл с 1 до k
  index <- sample(1:nrow(data),round(0.9*nrow(data))) # делим выборку 90/10%
  train.cv <- scaled[index,] # 90%
  test.cv <- scaled[-index,] # 10%
  
  test_data <- test.cv[, -which(names(test_) == "acceleration")]
  nn <- neuralnet(f,data=train.cv,hidden=c(5,2),linear.output=T) # обучаем нейр. сеть
  pr.nn <- neuralnet::compute(nn,test_data) # вычисляем прогнозы на тестовых данных
  pr.nn <- pr.nn$net.result*(max(data$acceleration)-min(data$acceleration))+min(data$acceleration) # обратная нормализация
  
  test.cv.r <- (test.cv$acceleration)*(max(data$acceleration)-min(data$acceleration))+min(data$acceleration) # обратная нормализация
  
  cv.error[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv) # сохраняем MSE текущего фолда
  
  pbar$step() # шаг для прогресс-бара
}
```

После чего были выведены средняя ошибка кросс-валидации и весь набор ошибок на каждом шаге:
```{r}
mean(cv.error) # считаем среднюю ошибку кросс-валидаци
cv.error # вывод полного вектора ошибок (ошибки каждого фолда)
```

```{r}
boxplot(cv.error,xlab='MSE CV',col='cyan',
        border='blue',names='CV error (MSE)',
        main='CV error (MSE) for NN',horizontal=TRUE)
```

График показывает разброс ошибок MSE для нейронной сети, где медиана находится ближе к 2.1, а диапазон значений распределен от приблизительно 1.6 до 2.8. Левая граница прямоугольника находится в районе 1.8, что соответствует первому квартилю распределения. Правая граница прямоугольника находится в районе 2.4, что соотвествутет третьему квартилю распределения.

# Вывод

В ходе данной лабораторной работы был произведён регрессионный анализ данных двумя разными методами, а именно с помощью линейной регрессионной модели и модели искусственной нейронной сети. Первая часть работы была выполнена в соответствии с обучающей статьёй, а во второй части был использован датасет `Car information dataset` с сайта https://www.kaggle.com.

В обоих частях работы нейронная сеть демонстрировала более высокие показатели точности предсказывания целевой переменной, что подтверждается значениями MSE и визуальной интерпретацией графиков в разделах 1.3 и 2.3. Устойчивость моделей была также проверена кросс-валидацией. 
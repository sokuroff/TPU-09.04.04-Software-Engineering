---
title: "Лабораторная работа 2. Решение задач классификации методами машинного обучения."
author: 'Сокуров Р.Е.'
date: "21.10.2024"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Задание

# Ход работы

## 1. Выполнение расчётов по статье.

### 1.1 Получение и разделение данных

Для начала, получили данные из датасета `iris` и разделили данные на обучающую и тестовую выборки в соотношении 80/20:


```{r}
set.seed(111) # установка зерна случайной генерации
# Получение данных
data("iris") # импорт датасета
str(iris) # вывод структуры датасета
summary(iris) # статистика всех переменных данных
```
```{r}
ind <- sample(2, nrow(iris), # делим датасет на две части по строкам
              replace = TRUE, # не уверен что это на что-то влияет
              prob = c(0.8, 0.2)) # делим части в отношении 80/20%
training <- iris[ind==1,] # 80% датасета в обучение
testing <- iris[ind==2,] # 20% в тест
```

### 1.2 Анализ датасета

Затем выполнили построение корелляционных матриц и матриц парных диаграмм (scatter plots):

```{r}
library(psych) # подключаем библиотеку, которая содержит
               # функции для анализа данных

pairs.panels(training[,-5], # строим scatter plots для всех
                            # переменных в датасете
                            # кроме пятого столбца (species)
             gap = 0,       # промежуток между диаграммами равен 0
             bg = c("red", "yellow", "blue")[training$Species],
             # задаём разные цвета точек для каждого вида цветка
             # setosa - красный, versicolor - жёлтый, virginica - синий
             pch=21) # стиль маркера: круглый, с заливкой фона
```

Здесь на основной диагонали располагаются переменные датасета (и их распределение), выше от главной диагонали — корреляции переменных между собой, а ниже — диаграммы рассеивания.

Длина и ширина лепестка обладают высокой корреляцией. Это означает, что чем длиньше лепесток, тем он шире. Помимо этого, высоким значением корреляции обладают длины чашелистиков и лепестков, а также длина чашелистика и ширина лепестка.

Красные линии на диаграммах рассеивания — это кривые тренда (LOESS). Они отражают как переменные связаны друг с другом, а также характер этой зависимости. Например, эти кривые могут быть не прямыми, что и отражает нелинейные зависимости между признаками.

Доверительные эллипсы чёрного цвета (эллипсы разброса/ковариации) охватывают область, где находится большая часть данных для каждого вида цветков. ТаК, если эллипс вытянут вдоль диагонали, это указывает на сильную положительную или отрицательную корреляцию между переменным, а если эллипс более округлый, это означает, что корреляция между переменными слабая или отсутствует. Также, чем больше эллипс, тем больше вариация данных для данных признаков. 

Как видно, в данном датасете несколько независимых переменных обладают мультиколлинеарностью. Это может привести к разным негативным последствиям, например:

1. Когда независимые переменные обладают высокой коллинеарностью, модель может неправильно оценить влияние каждого признака на результат;

2. Коэффициенты становятся менее значимыми в статистическом смысле, даже если на самом деле по отдельности эти переменные невероятно важны;

3. Модель может переобучитья и т.п.

Существует много разных способов борьбы с мультиколлинеарностью, например удалить одну из независимых переменных, использовать методы регуляризации и т.п.

В ходе данной работы был использован метод главных компонент (PCA - Principal Component Analysis) для преобразования корелированных переменных в новый набор компонент. 

### 1.3 Метод главных компонент (PCA - Principal Component Analysis)

Поскольку метод главных компонент базируется на независимых переменных, был убран 5й столбец из датасета:

```{r}
pc <- prcomp(training[,-5], # берём все независимые признаки
             center = TRUE, # центрируем данные
             scale. = TRUE) # масштабируем признаки
attributes(pc) # получаем атрибуты 
```
Далее был выполнен анализ результатов метода:
```{r}
print(pc)
```
Здесь стандартное отклонение показывает, какое значение дисперсии данных объясняется каждой компонентой. PC1 имеет значение 1.72 — объсняет наибольшую долю дисперсии, PC4 0.14 объясняет наименьшую долю. 

Затем отображается матрица нагрузок (rotation), которая показывает, как исходные признаки трансформируются в главные компоненты. Каждое значение здесь является коэффициентом, характеризующим вклад исходного признака в компоненту.

Далее был сформирован отчёт о важности каждой компоненты:

```{r}
d
```

Про стандартное отклонение уже было написано, а вот Proportion of Variance (Доля дисперсии) говорит о том, что 73.7% общей дисперсии объясняется PC1, ещё 22.1% — PC2. Таким образом лишь двумя компонентами объясняется уже 95,8% разброса данных.

Cumulative Proportion (Накопленная доля) показывает, что все четыре компоненты кумулятивно (вместе) объясняют 100% разброса данных.

Далее было рассмотрено, исчезла ли проблема мультиколлинеарности или нет.

### 1.4 Влияние метода главных компонент на мультиколлинеарность

Вновь были построены корелляционные матрицы и матрицы парных диаграмм (scatter plots):

```{r}
pairs.panels(pc$x,
             gap=0,
             bg = c("red", "yellow", "blue")[training$Species],
             pch=21)
```

Как видно из данного графика, проблема мультиколлинеарности полностью ушла. 

### 1.5 Построение Bi-Plot
```{r results = FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(devtools)
library(ggbiplot)
```
```{r}
g <- ggbiplot(pc, # создаём график, передаём туда PC
              obs.scale = 1, # размер наблюдений равен 1
              var.scale = 1, # размер переменных равен 1
              groups = training$Species, # точки на графике будут раскрашены в зависимости от группы (вида цветка)
              ellipse = TRUE, # доверительные эллипсы 
              circle = TRUE, # добавляет окружность радиусом 1 для точек
              ellipse.prob = 0.68) # уровень вероятности для эллипсов
g <- g + scale_color_discrete(name = '') # настройка цветовой шкалы без легенды
g <- g + theme(legend.direction = 'horizontal',
               legend.position = 'top') # корректировка отображения легенды
print(g)
```

Здесь по оси X расположили PC1, которая объясняет 73,7% дисперсии, а по Y — PC2, которая объсняет 22,1%. Эллипсы отображают области вероятного нахождения точек (68% всех точек определенного класса). Стрелки - переменные исходного набора данных, длина стрелки показывает важность этой переменной для компоненты, а угол стрелки относительно двух осей отображает вклад переменной в каждую из компонент (можно проекции построить). Видно, что кластер setosa чётко отделён от остальных, в то время как versicolor и virginica являются схожими, при использовании данных параметров. 

### 1.6 Предсказание с помощью главных компонент

```{r}
trg <- predict(pc, training) # преобразование обучающих данных с помощью PCA
trg <- data.frame(trg, training[5]) # формируем дата-фрейм с ответами (5й столбец)
tst <- predict(pc, testing) # преобразование также тестовой выборки
tst <- data.frame(tst, testing[5]) # формируем дата-фрейм с ответами (5й столбец)
```

Для предсказаний была использована модель многономиальной логистической регрессии, поскольку зависимая переменная имеет более двух категорий (три вида цветка). В качестве обучающих данных были использованы две первые компоненты, потому что именно они объясняют 95% разброса данных. 

```{r}
library(nnet)
trg$Species <- relevel(trg$Species, ref = "setosa")
mymodel <- multinom(Species~PC1+PC2, data = trg)
summary(mymodel)
```

### 1.7 Матрица ошибок (confusion matrix) и матрица неправильной классификации (misclassification error)
На обучающей выборке:

```{r}
p <- predict(mymodel, trg)
tab <- table(p, trg$Species)
tab
1 - sum(diag(tab))/sum(tab)
```

Ошибка неправильной классификации 6.7%.

На тестовой выборке:
```{r}
p1 <- predict(mymodel, tst)
tab1 <- table(p1, tst$Species)
tab1
1 - sum(diag(tab1))/sum(tab1)
```

Ошибка неправильной классификации 13%.
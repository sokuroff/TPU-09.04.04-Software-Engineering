---
title: "Лабораторная работа 2. Решение задач классификации методами машинного обучения."
author: 'Сокуров Р.Е.'
date: "25.10.2024"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Задание

# Ход работы

## 1. Выполнение расчётов по статье.

### 1.1 Получение и разделение данных

Для начала, получили данные из датасета `iris` и разделили данные на обучающую и тестовую выборки в соотношении 80/20:


```{r}
set.seed(111) # установка зерна случайной генерации
# Получение данных
data("iris") # импорт датасета
str(iris) # вывод структуры датасета
summary(iris) # статистика всех переменных данных
```
```{r}
ind <- sample(2, nrow(iris), # делим датасет на две части по строкам
              replace = TRUE,
              prob = c(0.8, 0.2)) # делим части в отношении 80/20%
training <- iris[ind==1,] # 80% датасета в обучение
testing <- iris[ind==2,] # 20% в тест
```

### 1.2 Анализ датасета

Затем выполнили построение корелляционных матриц и матриц парных диаграмм (scatter plots):

```{r}
library(psych) # подключаем библиотеку, которая содержит
               # функции для анализа данных

pairs.panels(training[,-5], # строим scatter plots для всех
                            # переменных в датасете
                            # кроме пятого столбца (species)
             gap = 0,       # промежуток между диаграммами равен 0
             bg = c("red", "yellow", "blue")[training$Species],
             # задаём разные цвета точек для каждого вида цветка
             # setosa - красный, versicolor - жёлтый, virginica - синий
             pch=21) # стиль маркера: круглый, с заливкой фона
```

Здесь на основной диагонали располагаются переменные датасета (и их распределение), выше от главной диагонали — корреляции переменных между собой, а ниже — диаграммы рассеивания.

Длина и ширина лепестка обладают высокой корреляцией. Это означает, что чем длиньше лепесток, тем он шире. Помимо этого, высоким значением корреляции обладают длины чашелистиков и лепестков, а также длина чашелистика и ширина лепестка.

Красные линии на диаграммах рассеивания — это кривые тренда (LOESS). Они отражают как переменные связаны друг с другом, а также характер этой зависимости. Например, эти кривые могут быть не прямыми, что и отражает нелинейные зависимости между признаками.

Доверительные эллипсы чёрного цвета (эллипсы разброса/ковариации) охватывают область, где находится большая часть данных для каждого вида цветков. ТаК, если эллипс вытянут вдоль диагонали, это указывает на сильную положительную или отрицательную корреляцию между переменным, а если эллипс более округлый, это означает, что корреляция между переменными слабая или отсутствует. Также, чем больше эллипс, тем больше вариация данных для данных признаков. 

Как видно, в данном датасете несколько независимых переменных обладают мультиколлинеарностью. Это может привести к разным негативным последствиям, например:

1. Когда независимые переменные обладают высокой коллинеарностью, модель может неправильно оценить влияние каждого признака на результат;

2. Коэффициенты становятся менее значимыми в статистическом смысле, даже если на самом деле по отдельности эти переменные невероятно важны;

3. Модель может переобучитья и т.п.

Существует много разных способов борьбы с мультиколлинеарностью, например удалить одну из независимых переменных, использовать методы регуляризации и т.п.

В ходе данной работы был использован метод главных компонент (PCA - Principal Component Analysis) для преобразования корелированных переменных в новый набор компонент. 

### 1.3 Метод главных компонент (PCA - Principal Component Analysis)

Поскольку метод главных компонент базируется на независимых переменных, был убран 5й столбец из датасета:

```{r}
pc <- prcomp(training[,-5], # берём все независимые признаки
             center = TRUE, # центрируем данные
             scale. = TRUE) # масштабируем признаки
attributes(pc) # получаем атрибуты 
```
Далее был выполнен анализ результатов метода:
```{r}
print(pc)
```
Здесь стандартное отклонение показывает, какое значение дисперсии данных объясняется каждой компонентой. PC1 имеет значение 1.72 — объсняет наибольшую долю дисперсии, PC4 0.14 объясняет наименьшую долю. 

Затем отображается матрица нагрузок (rotation), которая показывает, как исходные признаки трансформируются в главные компоненты. Каждое значение здесь является коэффициентом, характеризующим вклад исходного признака в компоненту.

Далее был сформирован отчёт о важности каждой компоненты:

```{r}
summary(pc)
```

Про стандартное отклонение уже было написано, а вот Proportion of Variance (Доля дисперсии) говорит о том, что 73.7% общей дисперсии объясняется PC1, ещё 22.1% — PC2. Таким образом лишь двумя компонентами объясняется уже 95,8% разброса данных.

Cumulative Proportion (Накопленная доля) показывает, что все четыре компоненты кумулятивно (вместе) объясняют 100% разброса данных.

Далее было рассмотрено, исчезла ли проблема мультиколлинеарности или нет.

### 1.4 Влияние метода главных компонент на мультиколлинеарность

Вновь были построены корелляционные матрицы и матрицы парных диаграмм (scatter plots):

```{r}
pairs.panels(pc$x,
             gap=0,
             bg = c("red", "yellow", "blue")[training$Species],
             pch=21)
```

Как видно из данного графика, проблема мультиколлинеарности полностью ушла. 

### 1.5 Построение Bi-Plot
```{r results = FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(devtools)
library(ggbiplot)
```
```{r}
g <- ggbiplot(pc, # создаём график, передаём туда PC
              obs.scale = 1, # размер наблюдений равен 1
              var.scale = 1, # размер переменных равен 1
              groups = training$Species, # точки на графике будут раскрашены в зависимости от группы (вида цветка)
              ellipse = TRUE, # доверительные эллипсы 
              circle = TRUE, # добавляет окружность радиусом 1 для точек
              ellipse.prob = 0.68) # уровень вероятности для эллипсов
g <- g + scale_color_discrete(name = '') # настройка цветовой шкалы без легенды
g <- g + theme(legend.direction = 'horizontal',
               legend.position = 'top') # корректировка отображения легенды
print(g)
```

Здесь по оси X расположили PC1, которая объясняет 73,7% дисперсии, а по Y — PC2, которая объсняет 22,1%. Эллипсы отображают области вероятного нахождения точек (68% всех точек определенного класса). Стрелки - переменные исходного набора данных, длина стрелки показывает важность этой переменной для компоненты, а угол стрелки относительно двух осей отображает вклад переменной в каждую из компонент (можно проекции построить). Видно, что кластер setosa чётко отделён от остальных, в то время как versicolor и virginica являются схожими, при использовании данных параметров. 

### 1.6 Предсказание с помощью многономиальной логистической регрессии

```{r}
trg <- predict(pc, training) # преобразование обучающих данных с помощью PCA
trg <- data.frame(trg, training[5]) # формируем дата-фрейм с ответами (5й столбец)
tst <- predict(pc, testing) # преобразование также тестовой выборки
tst <- data.frame(tst, testing[5]) # формируем дата-фрейм с ответами (5й столбец)
```

Для предсказаний была использована модель многономиальной логистической регрессии, поскольку зависимая переменная имеет более двух категорий (три вида цветка). В качестве обучающих данных были использованы две первые компоненты, потому что именно они объясняют 95% разброса данных. 

```{r}
library(nnet)
trg$Species <- relevel(trg$Species, ref = "setosa")
mymodel <- multinom(Species~PC1+PC2, data = trg)
summary(mymodel)
```

### 1.7 Матрица ошибок (confusion matrix) и матрица неправильной классификации (misclassification error)
На обучающей выборке:

```{r}
p <- predict(mymodel, trg)
tab <- table(p, trg$Species)
tab
1 - sum(diag(tab))/sum(tab)
```

Ошибка неправильной классификации 6.7%.

На тестовой выборке:
```{r}
p1 <- predict(mymodel, tst)
tab1 <- table(p1, tst$Species)
tab1
1 - sum(diag(tab1))/sum(tab1)
```

Ошибка неправильной классификации 13%.

## 2. Выполнение расчётов по выбранной теме.

### 2.1 Получение и разделение данных

Для самостоятельной части лабораторной работы был выбран датасет Human Activity Recognition Using Smartphones <https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones>. Данный датасет содержит информацию с различных датчиков смартфона (561 разных параметров), а задача состоит в следующем: необходимо корректно определить тип активности, которой занимался человек по информации с этих датчиков. Доступны 6 разных типов активности:

1. Ходьба;
2. Подъём по лестнице; 
3. Спуск по лестнице;
4. Сидение; 
5. Стояние;
6. Лежание.

Данные были предворительно предобработаны с помощью языка Python, а именно были собраны вместе названия столбцов, сами данные, ответы к ним (тип активности) и сохранены в файл data.csv. Импортируем его и разделим выборку в процентном соотношении 80/20:

```{r}
data <- read.csv("data.csv")
ind <- sample(2, nrow(data), # делим датасет на две части по строкам
              replace = TRUE, 
              prob = c(0.8, 0.2)) # делим части в отношении 80/20%
training <- data[ind==1,] # 80% датасета в обучение
testing <- data[ind==2,] # 20% в тест
```

### 2.2 Анализ датасета

Поскольку количество данных слишком велико (561 признак), то было принято решение вместо отрисовки графика создать корелляционную матрицу и сохранить её в файл excel. Он доступен в репозитории рядом с данным отчётом и называется `correlation_matrix.xlsx` (<https://clck.ru/3EBWcL>).

```{r}
library(corrplot)
cor_matrix <- cor(data)  # Формируем корреляционную матрицу

cor_df <- as.data.frame(cor_matrix) # сохраняем её как дата-фрейм

# Округляем значения для удобства
cor_df <- round(cor_df, 2)

# Добавляем столбец с названиями переменных
cor_df <- cbind(Variable = rownames(cor_df), cor_df)

library(writexl)
# Сохраняем корреляционную матрицу в файл Excel
write_xlsx(cor_df, "correlation_matrix.xlsx")
```

В этом датасете множество переменных обладают высокой коллинеарностью, а также его размерность слишком велика (561 признаков). Для борьбы с обоими проблемами был использован метод главных компонент. 

### 2.3 Метод главных компонент

Поскольку данный метод базируется на независимых переменных, был убран 562й столбец из датасета и сформирован отчёт о важности каждой компоненты (вывод ограничен до 17 строк):

```{r results = FALSE}
pc <- prcomp(training[,-562], # берём все независимые признаки
             center = TRUE, # центрируем данные
             scale. = TRUE) # масштабируем признаки
summary(pc)
```
```{r echo = FALSE}
output <- capture.output(summary(pc))
limited_output <- head(output, n = 17)  # Ограничиваем до 10 строк
cat(limited_output, sep = "\n")
```

Всего главных компонент больше 560, но уже после 28й компоненты они начинают описывать менее 1.5% разброса данных. Поэтому для дальнейшего анализа использовалось 28 главных компонент.

### 2.4 Влияние метода главных компонент на мультиколлинеарность

Для проверки влияния метода на мультиколлинеарность, была составлена матрица корреляции для 28 компонент:

```{r}
pc_data <- pc$x[, 1:28]  # Используем только первые 28 главных компонент

# Вычисляем корреляцию для главных компонент
cor_matrix <- cor(pc_data)

# Преобразуем в дата-фрейм и округляем значения
cor_df <- as.data.frame(round(cor_matrix, 2))

# Добавляем столбец с названиями переменных
cor_df <- cbind(Variable = rownames(cor_df), cor_df)

# Сохраняем корреляционную матрицу в файл Excel
write_xlsx(cor_df, "correlation_matrix_PC.xlsx")
```

Файл `correlation_matrix_PC.xlsx` доступен в репозитории рядом с данным отчётом (<https://clck.ru/3EBWcL>). Как видно из этого файла, теперь данные полностью независимы друг от друга.

### 2.5 Предсказание с помощью многономиальной логистической регрессии

```{r}
# Преобразуем обучающие данные с помощью PCA
trg <- predict(pc, training) 

# Формируем дата-фрейм с ответами (5й столбец)
trg <- data.frame(trg[, 1:28], Activity = training[, 562]) # Используем только первые 28 PC

# Преобразование тестовой выборки
tst <- predict(pc, testing) 
tst <- data.frame(tst[, 1:28], Activity = testing[, 562]) # Используем только первые 28 PC
```

Для предсказаний была использована модель многономиальной логистической регрессии, поскольку зависимая переменная имеет более двух категорий (6 видов активностей). 

```{r}
# Преобразуем переменную Activity в фактор
trg$Activity <- factor(trg$Activity)

# Устанавливаем референтное значение для целевой переменной
trg$Activity <- relevel(trg$Activity, ref = "1")

# Обучаем модель с использованием 28 главных компонент
mymodel <- multinom(Activity ~ ., data = trg) # Используем . для включения всех 75 PC
summary(mymodel)
```

### 2.6 Матрица ошибок (confusion matrix) и матрица неправильной классификации (misclassification error)
На обучающей выборке:

```{r}
p <- predict(mymodel, trg)
tab <- table(p, trg$Activity)
tab
1 - sum(diag(tab))/sum(tab)
```

Ошибка неправильной классификации 7.5%.

На тестовой выборке:
```{r}
p1 <- predict(mymodel, tst)
tab1 <- table(p1, tst$Activity)
tab1
1 - sum(diag(tab1))/sum(tab1)
```

Ошибка неправильной классификации 8.5%.

# Вывод

В ходе данной лабораторной работы был исследован и использован метод главных компонент для исправления мультиколлинеарности независимых переменных, а также, во второй части работы, для уменьшения размерности датасета.
Далее на основе получившихся независимых компонент была составлена модель многономиальной логистической регрессии для классификации данных на несколько классов. В самостоятельной части работы ошибка неправильной классификации на тестовом наборе составила 8.5%, что считается хорошим результатом. 

